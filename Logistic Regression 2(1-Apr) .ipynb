{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "\"\"\" \n",
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. \n",
    "As mentioned above, the performance of a model significantly depends on the value of hyperparameters. \n",
    "Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. \n",
    "Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "\"\"\"\n",
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "grid search cv : all possible compinations of paramenters to provide the most optimized model\n",
    "\n",
    "randomize search cv : random combinations over a numer of iterations and provides an optiized model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "\"\"\"\n",
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets. Typically, when splitting a data set into testing and training sets, the goal is to ensure that no data is shared between these two sets. Ideally, there is no intersection between these two sets. This is because the purpose of the testing set is to simulate the real-world data which is unseen to that model. However, when evaluating a model, we do have full access to both our train and test sets, so it is our duty to ensure that there is no overlapping between the training data and the testing data (i.e, no intersection).\n",
    "\n",
    "As a result, due to the Data leakage, we got unrealistically high levels of performance of our model on the test set, because that model is being run on data that it had already seen in some capacity in the training set. The model effectively memorizes the training set data and is easily able to correctly output the labels or values for those examples of the test dataset. Clearly, this is not ideal, as it misleads the person who evaluates the model. When such a model is then used on truly unseen data that is coming mostly on the production side, then the performance of that model will be much lower than expected after deployment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "\"\"\" \n",
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\"\n",
    "One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "\"\"\"\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "A confusion matrix is a table that is used to define the performance of a classification algorithm. A confusion matrix visualizes and summarizes the performance of a classification algorithm.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "\"\"\" Q6. Explain the difference between precision and recall in the context of a confusion matrix.\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\"\n",
    "Precision : TP/TP+FP\n",
    "\n",
    "Recall : TP/TP+FN\n",
    "\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "\"\"\" Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\"\n",
    "TP, TN, FP, FN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\"\"\" \n",
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\" \n",
    "Confusion matrices can be used to calculate performance metrics for classification models. Of the many performance metrics used, the most common are accuracy, precision, recall, and F1 score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "\"\"\" \n",
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\"\n",
    "Model accuracy is defined as the number of classifications a model correctly predicts divided by the total number of predictions made. It's a way of assessing the performance of a model, but certainly not the only way\n",
    "\n",
    "TP+TN/TP+FP+FN+TN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "\"\"\"\n",
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "\"\"\"\n",
    "\n",
    "#Ans\n",
    "\n",
    "\"\"\" \n",
    "performance metrics used, the most common are accuracy, precision, recall, and F1 score.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
