{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [0.25, 0.25],\n",
       "       [0.5 , 0.5 ],\n",
       "       [1.  , 1.  ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "\"\"\" \n",
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "# Min - Max Scaling\n",
    "\n",
    "Feature Scaling technique that changes the range of the distrbution between 0,1.\n",
    "It is used to reduce the range in an limited range and it makes it easy to run ML Algoriths\n",
    "\n",
    "Min-max scaling is similar to z-score normalization in that it will replace every value in a column with a new value using a formula. In this case, that formula is:\n",
    "\n",
    "m = (x -xmin) / (xmax -xmin)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11785113 0.1767767  0.29462783 0.35355339 0.41247896 0.23570226\n",
      "  0.47140452 0.41247896 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "\"\"\" \n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "We use unit vector technique to rescale the features using its unit vector.\n",
    "The feature is rescaled to magnitude of one. \n",
    "Where Min-max scaling is similar to z-score normalization in that it will replace every value in a column with a new value using a formula.\n",
    "Feature Scaling technique that changes the range of the distrbution between 0,1.\n",
    "\n",
    "where \n",
    "\n",
    "x'= x/||x||\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "x_array = np.array([2,3,5,6,7,4,8,7,6])\n",
    "normalized_arr = preprocessing.normalize([x_array])\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "\"\"\" \n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\"\"\"\n",
    "\n",
    "# Answers\n",
    "\n",
    "\"\"\"  \n",
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. \n",
    "It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. \n",
    "These new transformed features are called the Principal Components. \n",
    "It is one of the popular tools that is used for exploratory data analysis and predictive modeling. \n",
    "It is a technique to draw strong patterns from the given dataset by reducing the variances.\n",
    "\n",
    "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. \n",
    "Some real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels. \n",
    "It is a feature extraction technique, so it contains the important variables and drops the least important variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "\"\"\" \n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. \n",
    "Some real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels. \n",
    "It is a feature extraction technique, so it contains the important variables and drops the least important variable.\n",
    "\n",
    "PCA is mainly used as the dimensionality reduction technique in various AI applications such as computer vision, image compression, etc.\n",
    "It can also be used for finding hidden patterns if data has high dimensions. Some fields where PCA is used are Finance, data mining, Psychology, etc.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "\"\"\" \n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "In my opinion, Rating, Price and Delevery time are continous variables spread over an very large range and we can apply min max scaling to bring all in an equivalent standards,\n",
    "\n",
    "Min-max scaling is similar to z-score normalization in that it will replace every value in a column with a new value using a formula. In this case, that formula is:\n",
    "\n",
    "m = (x -xmin) / (xmax -xmin)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "\"\"\" \n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\"\"\"\n",
    "\n",
    "#Answrs\n",
    "\n",
    "\"\"\" \n",
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. \n",
    "It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. \n",
    "These new transformed features are called the Principal Components. \n",
    "It is one of the popular tools that is used for exploratory data analysis and predictive modeling. \n",
    "It is a technique to draw strong patterns from the given dataset by reducing the variances.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "\"\"\" \n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[1, 5, 10, 15, 20]]\n",
    "data=np.array(data)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(data)\n",
    "scaler.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "\"\"\" \n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\" \n",
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. \n",
    "It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. \n",
    "These new transformed features are called the Principal Components. \n",
    "It is one of the popular tools that is used for exploratory data analysis and predictive modeling. \n",
    "It is a technique to draw strong patterns from the given dataset by reducing the variances.\n",
    "\n",
    "We can collapse this many features into some few significant dimensions to improve effeciency\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
