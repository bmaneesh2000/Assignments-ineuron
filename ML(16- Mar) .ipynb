{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "\"\"\" Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated? \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "#Overfitting :- Overfitting is an undesirable machine learning behavior that occurs when \n",
    "the machine learning model gives accurate predictions for training data but not for new data. \n",
    "When data scientists use machine learning models for making predictions, they first train the model on a known data set. \n",
    "Then, based on this information, the model tries to predict outcomes for new data sets. \n",
    "An overfit model can give inaccurate predictions and cannot perform well for all types of new data.\n",
    "\n",
    "#Underfitting :- Underfitting is a scenario in data science where a data model is unable to capture the \n",
    "relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. \n",
    "It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. \n",
    "Like overfitting, when a model is underfitted,it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. \n",
    "If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. \n",
    "Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.\n",
    "\n",
    "They can be adjusted hy hyperparamenter tuning and changing the bias and variance\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "\"\"\" Q2: How can we reduce overfitting? Explain in brief. \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "-Cross-validation :- Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "\n",
    "-Train with more data :- It wont work every time, but training with more data can help algorithms detect the signal better. I\n",
    "\n",
    "-Remove features :- you can manually improve their generalizability by removing irrelevant input features\n",
    "\n",
    "-Early stopping :- When youre training a learning algorithm iteratively, you can measure how well each iteration of the model performs. Up until a certain number of iterations, new iterations improve the model. \n",
    "After that point, however, the models ability to generalize can weaken as it begins to overfit the training data. Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "-Regularization :- Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "The method will depend on the type of learner youre using.For example, you could prune a decision tree, use dropout on a neural network, \n",
    "or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "-Ensembling:- Ensembles are machine learning methods for combining predictions from multiple separate models. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "\"\"\"\n",
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, \n",
    "i.e., it only performs well on training data but performs poorly on testing data. \n",
    "Underfitting destroys the accuracy of our machine learning model. \n",
    "Its occurrence simply means that our model or the algorithm does not fit the data well enough. \n",
    "It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. \n",
    "\n",
    "\n",
    "\n",
    "Reasons for Underfitting:\n",
    "\n",
    "High bias and low variance \n",
    "The size of the training dataset used is not enough.\n",
    "The model is too simple.\n",
    "Training data is not cleaned and also contains noise in it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "\"\"\" \n",
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. \n",
    "If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. \n",
    "In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm cant be more complex and less complex at the same time.\n",
    "The best fit will be given by hypothesis on the tradeoff point.\n",
    "\n",
    "If the model has low bias and high variance it is overfitted and if the model is low bias and low variance the model is underfitted\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "\"\"\" \n",
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\"\"\"\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "-Reasons for Underfitting:\n",
    "\n",
    "High bias and low variance \n",
    "The size of the training dataset used is not enough.\n",
    "The model is too simple.\n",
    "Training data is not cleaned and also contains noise in it.\n",
    "\n",
    "-Reasons for Overfitting are as follows:\n",
    "\n",
    "High variance and low bias \n",
    "The model is too complex\n",
    "The size of the training data \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "\"\"\" \n",
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "Bias\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. \n",
    "Being high in biasing gives a large error in training as well as testing data. \n",
    "Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "\n",
    "Variance\n",
    "The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. \n",
    "The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasnt seen before. \n",
    "As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. \n",
    "In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "\"\"\" \n",
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it.\n",
    "Sometimes the machine learning model performs well with the training data but does not perform well with the test data. \n",
    "It means the model is not able to predict the output when deals with unseen data by introducing noise in the output, and hence the model is called overfitted. \n",
    "This problem can be deal with the help of a regularization technique.\n",
    "This technique can be used in such a way that it will allow to maintain all variables or features in the model by reducing the magnitude of the variables. \n",
    "Hence, it maintains accuracy as well as a generalization of the model.\n",
    "\n",
    "There are mainly two types of regularization techniques, which are given below:\n",
    "\n",
    "Ridge Regression\n",
    "Lasso Regression\n",
    "\n",
    "-Ridge Regression\n",
    "\n",
    "Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
    "Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n",
    "In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. \n",
    "We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "\n",
    "-Lasso Regression:\n",
    "\n",
    "Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
    "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
    "Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
    "It is also called as L1 regularization.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
