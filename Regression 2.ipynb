{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f933c8b-f301-4276-a58b-1f41fec033e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "\"\"\" Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent? \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable in a regression model. \n",
    "Whereas correlation explains the strength of the relationship between an independent and dependent variable, \n",
    "R-squared explains the extent to which the variance of one variable explains the variance of the second variable. \n",
    "So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417fa3c-5918-44c1-953c-263190c895bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "\"\"\" Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\" \n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected. \n",
    "Typically, the adjusted R-squared is positive, not negative. It is always lower than the R-squared.\n",
    "\n",
    "Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. \n",
    "This is called overfitting and can return an unwarranted high R-squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d871c27-0d48-40cf-8b30-de6c679ab49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "\"\"\" Q3. When is it more appropriate to use adjusted R-squared? \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\" \n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected. \n",
    "Typically, the adjusted R-squared is positive, not negative. It is always lower than the R-squared.\n",
    "\n",
    "Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. \n",
    "This is called overfitting and can return an unwarranted high R-squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7ec53-91ad-4e45-b533-d9616ec35d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "\"\"\" Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent? \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
    "MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "RMSE (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
    "R-squared (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddeda93-47b2-4a89-aa8c-311fb1b6a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "\"\"\" Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
    "MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "RMSE (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
    "R-squared (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5cf56-a541-442b-a49b-3d1000e9db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "\"\"\" Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use? \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. \n",
    "As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss. \n",
    "The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.\n",
    "\n",
    "We use a diffrent formula that uses abs isntead of square,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403e4dc-f1b5-4b98-bb0f-ad48c0f39de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "\"\"\" \n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? \n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "In Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. \n",
    "We also add a coefficient  \\lambda  to control that penalty term. \n",
    "In this case if  \\lambda  is zero then the equation is the basic OLS else if  \\lambda \\, > \\, 0 then it will add a constraint to the coefficient. \n",
    "As we increase the value of \\lambda this constraint causes the value of the coefficient to tend towards zero. \n",
    "This leads to tradeoff of higher bias (dependencies on certain coefficients tend to be 0 and on certain coefficients tend to be very large, \n",
    "making the model less flexible) for lower variance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb2e70-64f0-438e-8c56-eddec954fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "\"\"\"\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Limitation of Ridge Regression: Ridge regression decreases the complexity of a model but does not reduce the number of variables \n",
    "since it never leads to a coefficient been zero rather only minimizes it. Hence, this model is not good for feature reduction.\n",
    "\n",
    "Limitation of Lasso Regression:\n",
    "Lasso sometimes struggles with some types of data. If the number of predictors (p) is greater than the number of observations (n), Lasso will pick at most n predictors as non-zero, even if all predictors are relevant (or may be used in the test set).\n",
    "If there are two or more highly collinear variables then LASSO regression select one of them randomly which is not good for the interpretation of data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601bad2a-3a59-4b1e-913a-cf14f269454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "\"\"\"\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric? \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "I woud prefer model 2 as in model 1 RMSE is more prone to outliers and tends to give distorted results if outliers exist, and if model is too good t may be due to overfitting\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff75f9f-8835-465b-8e39-f4288ad64e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "\"\"\"\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\"\n",
    "I would choose model with lasso regression, as it would reduce more overfitting  due to lsso and also assist in feature selection more\n",
    "\n",
    "Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\n",
    "Lasso regression helps to reduce the overfitting in the model as well as feature selection.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
