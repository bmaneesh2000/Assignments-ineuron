{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "\"\"\" Q1. What is the mathematical formula for a linear SVM? \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.\n",
    "\n",
    "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n",
    "\n",
    "Line = W^T+b\n",
    "W^T+b = -1\n",
    "W^T+b = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "\"\"\" Q2. What is the objective function of a linear SVM? \"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "\"\"\" \n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points. To separate the two classes of data points, there are many possible hyperplanes that could be chosen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "\"\"\" Q3. What is the kernel trick in SVM? \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions. With that saying, the application of the kernel trick is not limited to the SVM algorithm. Any computations involving the dot products (x, y) can utilize the kernel trick.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "\"\"\" Q4. What is the role of support vectors in SVM Explain with example \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\" \n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "\"\"\" Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM? \"\"\"\n",
    "\n",
    "#Answer\n",
    "\n",
    "\"\"\"\n",
    "Hyperplane is an line that is used to sperate 2 categories of classes\n",
    "\n",
    "Marrginal line is the best fit line for the classificaion\n",
    "\n",
    "Soft margin is when there is some overlapping and error permissible\n",
    "\n",
    "Hard margin is when there is No overlaping and error\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "\"\"\" \n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries dof the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Answers\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X,y = datasets.load_iris(return_X_y = True , as_frame = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 2, 0, 1, 0, 2, 1, 0, 2, 2, 2, 2, 1, 1, 0, 1, 1, 0,\n",
       "       2, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "yp=clf.predict(X_test)\n",
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
